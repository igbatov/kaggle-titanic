
In this section we will use GLM to make predictions and will not think about interpretability of the model. 

First of all, lets impute all NA data.

```{r}
# create data from scratch
train <- read.csv('train.csv',na.strings=c(""))
test <- read.csv('test.csv',na.strings=c(""))

cTrain = preprocessTitanic(train);
cTest = preprocessTitanic(test);

# remove Surname as predict.cv.glm cannot work with uknown factors in test set
cTrain = subset(cTrain, select=-c(Surname))
cTest = subset(cTest, select=-c(Surname))

# check that we have no NA data
sapply(cTrain, function(x) sum(is.na(x)))

# split cTrain on train and test
set.seed(726) 
inTrain <- createDataPartition(cTrain$Survived, p = 0.7, list = FALSE)
cTrainTrain <- cTrain[inTrain, ]
cTrainTest <- cTrain[-inTrain, ]
```

Note, that though we didn't use CabinLetter in determing reliabile predictors, we can still use it for overall predictions because [multicollinearity do not make predictions of lm worse](http://blog.minitab.com/blog/adventures-in-statistics/what-are-the-effects-of-multicollinearity-and-when-can-i-ignore-them), it just concerns interpretation of predictors coefficients. 

Let's try to make GLM with all avaliable predictors.
```{r eval=FALSE}
# this gives perfect split error
glmfit = glm(Survived~., data=cTrainTrain, family=binomial);
pr = predict(glmfit, newdata=cTrainTest[,-1], type="class")
str(pr)
confusionMatrix(cTrainTest$Survived, pr)

```

Gives perfect split warning. Try penalized GLM (Firthâ€™s penalized-likelihood logistic regression) instead.
```{r eval=FALSE}
# this takes too long too compute
#glmfit = logistf(Survived~., data=cTrainTrain, family=binomial);
```

Logistf worked too long, try to regularize variables instead.
```{r}
cTest$Title[cTest$Title=='Dona'] = 'Miss'
# update levels
cTest$Title = factor(cTest$Title)

set.seed(1)
x = model.matrix(Survived~.,cTrainTrain)[,-1]
y = cTrainTrain$Survived
newx = model.matrix(Survived~.,cTrainTest)[,-1]

createCSV = function(model, newData, PassengerId, filename){
  pr = predict(model, newdata = newData)
  write.csv(data.frame(PassengerId = PassengerId, Survived = pr), file = filename)
}

createRidge = function(x,y,newData,PassengerId,filename){
  ridgefit = cv.glmnet(x,y,alpha=0, family='binomial', nfolds = 10)
  pr = predict.cv.glmnet(ridgefit, newx=newx, s="lambda.min", type="class")
  createCSV(ridgefit,newData,PassengerId,filename);
}

createLasso = function(x,y,newData,PassengerId,filename){
  lassofit = cv.glmnet (x,y,alpha=1, family='binomial', nfolds = 10)
  pr = predict.cv.glmnet(lassofit, newx=newx, s="lambda.min", type="class")
  createCSV(lassofit,newData,PassengerId,filename);
}

createCaret = function(data,newData,PassengerId,filename){
  # caret tunes alpha as well as lambda http://stats.stackexchange.com/questions/69638/does-caret-train-function-for-glmnet-cross-validate-for-both-alpha-and-lambda
  glmnet_ctrl <- trainControl(method = "cv", number = 10)
#  glmnet_fit <- train(Survived ~ ., data = data, method = "glmnet", preProcess = c("scale"), trControl = glmnet_ctrl)
  glmnet_fit <- train(Survived ~ ., data = data, method = "glmnet", trControl = glmnet_ctrl)
  pr = predict(glmnet_fit, newdata = newData)
  createCSV(glmnet_fit,newData,PassengerId,filename);
}

createCaret(cTrain[cTrain$Sex == 'male',], cTest[cTest$Sex == 'male',],test[test$Sex == 'male', ]$PassengerId,'TitanicRidgeMale.csv');

createCaret(cTrain[cTrain$Sex == 'female',], cTest[cTest$Sex == 'female',],test[test$Sex == 'female', ]$PassengerId,'TitanicRidgeFemale.csv');

```
