---
title: "Titanic data exploration using GLM, KNN, SVN, xgboost, random forests"
author: "Igor Batov"
date: "August 28, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading libraries

```{r}
# for glmnet()
library(glmnet)

# cv.glm (cross-validation of GLM)
library(boot)

# logistf()
# library(logistf)

# lda()
library(MASS)

# impute with missForest()
library(missForest)

# createDataPartition()
library(caret)

# for missmap()
library(Amelia)

# for corvif() function
source("HighstatLibV6.R")

# plotPredictors()
source('plotPredictors.R')

# for ggpairs() function
library(GGally)

## Preprocess funciton
preprocessTitanic = function (data){
  if("Survived" %in% colnames(data)){
    data$Survived = factor(data$Survived);
  } 
  data$Pclass = factor(data$Pclass);
  cData = subset(data, select=-c(PassengerId,Ticket))
  ## Create new predictors
  #  - extract Deck from Cabin
  cData$CabinLetter = factor(substr(cData$Cabin, 1, 1))
  #  - extract cabin number from Cabin
  cData$CabinNumber = factor(as.numeric(substr(cData$Cabin, 2, 4)))
  #  - extract title
  cData$Title = factor(gsub('(.*, )|(\\..*)', '', cData$Name))
  # - extract Surnames
  cData$Surname <- factor(sapply(cData$Name,function(x){x=as.character(x); strsplit(x, split='[,.]')[[1]][1];}))
  
  # impute variables with random forest
  # remove factors with more than 53 levels as missForest don't like it
  mf = missForest(subset(cData,select=-c(Surname,Name,CabinNumber,Cabin)))
  # check error
  mf$OOBerror
  # here we must add to imputed data skipped factors (except for CabinNumber - it has many NAs)
  cData = cbind(mf$ximp,subset(cData,select=c(Surname)))
  return (cData)
}
```
## GLM for interpretation
```{r child = 'GLM.Rmd'}
```

## Using GLM for prediction
```{r child = 'Ridge.Rmd'}
```

## Final prediction

## Conclusion
There is an opinion that for data with small number of observation we should minimize variance and thus select models with many constrains, such as lm, glm, lda. For data with many observations, we can guess that sample distribution is close to real one and thus we just need to fit it well (and minimize bias instead of variance), so tree models, QDA, KNN and others that tend to overfit gives better results.
