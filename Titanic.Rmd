---
title: "Titanic data exploration using GLM, KNN, SVN, xgboost, random forests"
author: "Igor Batov"
date: "August 28, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading and cleaning data.
First of all we want to extract the most influental predictors from GLM point of view.
Craft some new predictors using our common sense and create subset that has all predictors filled with data. 

```{r}
# for glmnet()
library(glmnet)

# cv.glm (cross-validation of GLM)
library(boot)

# logistf()
library(logistf)

# lda()
library(MASS)

# impute with missForest()
library(missForest)

# createDataPartition()
library(caret)

# for missmap()
library(Amelia)

# for corvif() function
source("HighstatLibV6.R")

# plotPredictors()
source('plotPredictors.R')

# for ggpairs() function
library(GGally)
```



```{r}
# Load data
train <- read.csv('train.csv',na.strings=c(""))
test <- read.csv('test.csv',na.strings=c(""))
# cast right datatypes on test
cTest$Pclass = factor(cTest$Pclass)

# 891 obs.
dim(train)
# plot missing variables
missmap(train);
# check for variables with bad predictive power
nearZeroVar(train,saveMetrics= TRUE)
# check how many NA values each predictor has
sapply(train, function(x) sum(is.na(x)))
# check how many unique valies each predictor has
sapply(train, function(x) length(unique(x)))

# Clean #1 
# -  remove NA cells
cTrain = na.omit(train)
# 891 obs.
dim(cTrain)
# - remove unnecesarry predictors
cTrain = subset(cTrain, select = -c(PassengerId,Ticket) )
# - set right predictor types
str(cTrain)
cTrain$Survived = factor(cTrain$Survived)
cTrain$Pclass = factor(cTrain$Pclass)

## Create new predictors
#  - extract Deck from Cabin
cTrain$CabinLetter = factor(substr(cTrain$Cabin, 1, 1))
#  - extract cabin number from Cabin
cTrain$CabinNumber = factor(as.numeric(substr(cTrain$Cabin, 2, 4)))
#  - extract title
cTrain$Title = factor(gsub('(.*, )|(\\..*)', '', cTrain$Name))
# - extract Surnames
cTrain$Surname <- factor(sapply(cTrain$Name, function(x){x=as.character(x); strsplit(x, split = '[,.]')[[1]][1];}))
```

## 

```{r}
# Clean #2 
# - remove all rows with empty Deck
ccTrain = subset(cTrain, CabinLetter != '')
dim(ccTrain)
# - remove all not-numeric CabinNumber
ccTrain = subset(ccTrain, !is.na(CabinNumber))
dim(ccTrain)
# - remove all not-numeric CabinNumber
ccTrain = subset(ccTrain, Embarked != '')
dim(ccTrain)
# - make CabinNumber to be a factor
ccTrain$CabinNumber = factor(ccTrain$CabinNumber);
```

Ok, we have trimmed out dataset from 891 observations to 176. The most truncating predictor was the CabinLetter. Using GLM we will try to determine if CabinLetter is important and if there is need to impute it or we can safely ignore it completely.

```{r}
glmfit = glm(Survived~.,data=subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber)),family=binomial);
```
This gives us Error. Lets look what variables can cause this. The CabinNumber looks suspicious with 86 levels from 176 observations.
```{r}
levels(ccTrain$CabinNumber)
```
Let's exclude this from predictors.

```{r}
glmfit = glm(Survived~.,data=subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber)),family=binomial);

summary(glmfit)
```
This gives no confident coefficients for any variable. This can be because of predictors multicollinearity. To quote [A protocol for data exploration to avoid common statistical problems](http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2009.00001.x/full) "If collinearity is ignored, one is likely to end up with a confusing statistical analysis in which nothing is significant, but where dropping one covariate can make the others significant, or even change the sign of estimated parameters." Lets check variance inflation factor (VIFs).

```{r}
corvif(subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber)))
```
Shows big GVIF (>4) for Pclass, Sex, CabinLetter, Title. Alain F. Zuur suggested to just drop predictors with high VIFs from the highest one by one.
```{r}
corvif(subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber,Title)))
```
Now dropping CabinLetter.
```{r}
corvif(subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber,Title, CabinLetter)))
```
Now all looks well in terms of VIFs.
```{r}
glmfit = glm(Survived~.,data=subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber,Title,CabinLetter)),family=binomial);
summary(glmfit)
```
Much better.

We saw that there is no use in columns Surname,Cabin,Name,CabinNumber,Title,CabinLetter for glm model that we want to interpret. Let's create glm ignoring these columns and using whole training data and try to check what predictors are the most reliable.
```{r}
cTrain = subset(train,select=-c(Cabin,Name,Ticket,PassengerId))
#impute Age - this will give us extra 0.01 in prediction accuaracy
cTrain$Age[is.na(cTrain$Age)] <- mean(cTrain$Age,na.rm=T)
cTrain = na.omit(cTrain)
dim(cTrain)
cTrain$Survived = factor(cTrain$Survived)
cTrain$Pclass = factor(cTrain$Pclass)
glmfit = glm(Survived~.,data=cTrain,family=binomial);
summary(glmfit)
```
We see that the reliable predictors are the same as those we saw on partial data glm plus Pclass and SibSp
VIFs are also ok for the whole data.
```{r}
corvif(cTrain)
```

It is also useful to make predictors vs response plot and look for visual trends and non-linearities. Again, quoting [A protocol for data exploration to avoid common statistical problems](http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2009.00001.x/full) "Note that the absence of clear patterns does not mean that there are no relationships; it just means that there are no clear two-way relationships. A model with multiple explanatory variables may still provide a good fit."
```{r}
plotPredictors('Survived',cTrain)
``` 

Plot glm predictions to interpret every predictor.

```{r}
library(effects)
plot(allEffects(glmfit))
```

Finally get confusion matrix for this model
```{r}
# split cTrain on train and test
set.seed(726) 
inTrain <- createDataPartition(cTrain$Survived, p = 0.7, list = FALSE)
cTrainTrain <- cTrain[inTrain, ]
cTrainTest <- cTrain[-inTrain, ]
# check how data is splitted
table(cTrainTrain$Survived)
table(cTrainTest$Survived)
# make model fit on train data
glmfit = glm(Survived~.,data=cTrainTrain,family=binomial);
# predict test data
pr = factor(ifelse(predict(glmfit, newdata = cTrainTest) > 0.5,1,0))
confusionMatrix(cTrainTest$Survived, pr)
```
Тотаl accuracy is 0.8233. Accuracy in predicting those who survived is 83% (68/81). Accuracy in predicting those who not survived is 81% (151/185). We can say that sensitivity is 83% while specificity is 81%.

Let's use more exact method of estimating test error - cross-validation.
```{r}
# Use 10-fold cross-validation
glmfit = glm(Survived~.,data=cTrain,family=binomial);
error = cv.glm(cTrain, glmfit,K=10)
# standard k-fold CV estimate
error$delta [1]
# bias corrected version k-fold CV estimate
error$delta [2]
```
So according to CV accuracy is 0.8565378. Not bad.
We can also consider interaction of predictors.
```{r}
# Stepwise Regression
library(MASS)
glmfit = glm(Survived~.^2, cTrain, family=binomial);
step <- stepAIC(glmfit, direction="both")
step$anova # display results

# return initial layout
par(mfrow=c(1,1))
error = cv.glm(cTrain, glmfit,K=10)
# standard k-fold CV estimate
error$delta [1]
# bias corrected version k-fold CV estimate
error$delta [2]
```
Decrease in CV error is small and we get warnings about possible overfit. In addition I am not sure how to interpret this interactions. So we stop on version without interactions.

I should note that because logistic regression doesn't have a specific target function, it is difficult to diagnose the fit. (It is possible to define "residuals" for logistic regression but they are difficult to interpret)[http://alumni.media.mit.edu/~tpminka/courses/36-350.2001/lectures/day31/]. 
So we will end up with conclusion that the significant predictors are Sex, Age, Pclass, SibSp. With slope and confidence intervals showed above.

## Using GLM for prediction
In this section we will use GLM to make predictions and will not think about interpretability of the model. 

First of all, lets impute all NA data.
```{r eval=FALSE}
# create data from scratch
train <- read.csv('train.csv',na.strings=c(""))
train$Survived = factor(train$Survived);
train$Pclass = factor(train$Pclass);
cTrain = subset(train, select=-c(PassengerId,Ticket))
## Create new predictors
#  - extract Deck from Cabin
cTrain$CabinLetter = factor(substr(cTrain$Cabin, 1, 1))
#  - extract cabin number from Cabin
cTrain$CabinNumber = factor(as.numeric(substr(cTrain$Cabin, 2, 4)))
#  - extract title
cTrain$Title = factor(gsub('(.*, )|(\\..*)', '', cTrain$Name))
# - extract Surnames
cTrain$Surname <- factor(sapply(cTrain$Name,function(x){x=as.character(x); strsplit(x, split='[,.]')[[1]][1];}))

# impute variables with random forest
# remove factors with more than 53 levels as missForest don't like it
mf = missForest(subset(cTrain,select=-c(Surname,Name,CabinNumber,Cabin)))
# check error
mf$OOBerror
# add to imputed data skiped columns (except for CabinNumber - it has many NAs)
cTrain = cbind(mf$ximp,subset(cTrain,select=c(Surname)))
# check that we have no NA data
sapply(cTrain, function(x) sum(is.na(x)))
```

Note, that though we didn't use CabinLetter in determing reliabile predictors, we can still use it for overall predictions because [multicollinearity do not make predictions of lm worse](http://blog.minitab.com/blog/adventures-in-statistics/what-are-the-effects-of-multicollinearity-and-when-can-i-ignore-them), it just concerns interpretation of predictors coefficients. 

Let's try to make GLM with all avaliable predictors.
```{r eval=FALSE}
# split cTrain on train and test
set.seed(726) 
inTrain <- createDataPartition(cTrain$Survived, p = 0.7, list = FALSE)
cTrainTrain <- cTrain[inTrain, ]
cTrainTest <- cTrain[-inTrain, ]
# this gives perfect split error
glmfit = glm(Survived~., data=cTrainTrain, family=binomial);
```
Gives perfect split warning. Try penalized GLM (Firth’s penalized-likelihood logistic regression) instead.
```{r eval=FALSE}
# this takes too long too compute
#glmfit = logistf(Survived~., data=cTrainTrain, family=binomial);
```
Penalize work too long, try lasso instead.
```{r eval=FALSE}
x = model.matrix(Survived~.,cTrainTrain)[,-1]
y = cTrainTrain$Survived
set.seed(1)
ridgefit = cv.glmnet (x,y,alpha=0, family='binomial', nfolds = 10)
pr = predict.cv.glmnet(ridgefit, newx, s="lambda.min", type="class")
confusionMatrix(cTrainTest$Survived, pr)

# coefficients of Ridge
coef(ridgefit)
```
Not much accuracy for Ridge. Let's try Lasso.
```{r}
set.seed(1)
ridgefit = cv.glmnet (x,y,alpha=1, family='binomial', nfolds = 10)
pr = predict.cv.glmnet(ridgefit, newx, s="lambda.min", type="class")
confusionMatrix(cTrainTest$Survived, pr)

# coefficients of Lasso
coef(ridgefit)
```
Much better accuracy.
Let's look which variables Lasso choose as non-zero.


## Conclusion
There is an opinion that for data with small number of observation we should minimize variance and thus select models with many constrains, such as lm, glm, lda. For data with many observations, we can guess that sample distribution is close to real one and thus we just need to fit it well (and minimize bias instead of variance), so tree models, QDA, KNN and others that tend to overfit gives better results.
