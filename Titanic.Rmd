---
title: "Titanic data exploration using GLM, LDA, SVN, random forests"
author: "Igor Batov"
date: "August 28, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading and cleaning data.
First of all we want to extract the most influental predictors from GLM point of view.
Craft some new predictors using our common sense and create subset that has all predictors filled with data. 

```{r}

library(caret)

# for missmap()
library(Amelia)

# for corvif() function
source("http://www.highstat.com/Book2/HighstatLibV6.R")

# for ggpairs() function
library(GGally)

# Load data
train <- read.csv('train.csv',na.strings=c(""))
# 891 obs.
dim(train)
# plot missing variables
missmap(train);
# check for variables with bad predictive power
nearZeroVar(train,saveMetrics= TRUE)
# check how many NA values each predictor has
sapply(training.data.raw,function(x) sum(is.na(x)))
# check how many unique valies each predictor has
sapply(training.data.raw, function(x) length(unique(x)))

# Clean #1 
# -  remove NA cells
cTrain = na.omit(train)
# 891 obs.
dim(cTrain)
# - remove unnecesarry predictors
cTrain = subset(cTrain, select = -c(PassengerId,Ticket) )
# - set right predictor types
str(cTrain)
cTrain$Survived = factor(cTrain$Survived)
cTrain$Pclass = factor(cTrain$Pclass)

## Create new predictors
#  - extract Deck from Cabin
cTrain$CabinLetter = factor(substr(cTrain$Cabin, 1, 1))
#  - extract cabin number from Cabin
cTrain$CabinNumber = as.numeric(substr(cTrain$Cabin, 2, 4))
#  - extract title
cTrain$Title = factor(gsub('(.*, )|(\\..*)', '', cTrain$Name))
# - extract Surnames
cTrain$Surname <- factor(sapply(cTrain$Name, function(x){x=as.character(x); strsplit(x, split = '[,.]')[[1]][1];}))
```

## 

```{r}
# Clean #2 
# - remove all rows with empty Deck
ccTrain = subset(cTrain, CabinLetter != '')
dim(ccTrain)
# - remove all not-numeric CabinNumber
ccTrain = subset(ccTrain, !is.na(CabinNumber))
dim(ccTrain)
# - remove all not-numeric CabinNumber
ccTrain = subset(ccTrain, Embarked != '')
dim(ccTrain)
# - make CabinNumber to be a factor
ccTrain$CabinNumber = factor(ccTrain$CabinNumber);
```

Ok, we have trimmed out dataset from 891 observations to 176. The most truncating predictor was the CabinLetter. Using GLM we will try to determine if CabinLetter is important and there is need to impute it or we can safely ignore it completely.

```{r}
glmfit = glm(Survived~.,data=subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber)),family=binomial);
```
This gives use Error. Lets look what variables can cause this. The suspicious looks CabinNumber with 86 levels from 176 observations.
```{r}
levels(ccTrain$CabinNumber)
```
Let's exclude this from predictors.

```{r}
glmfit = glm(Survived~.,data=subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber)),family=binomial);

summary(glmfit)
```
This gives no confident coefficients for any variable. This can be because of predictors multicollinearity. To quote [A protocol for data exploration to avoid common statistical problems](http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2009.00001.x/full) "If collinearity is ignored, one is likely to end up with a confusing statistical analysis in which nothing is significant, but where dropping one covariate can make the others significant, or even change the sign of estimated parameters." Lets check variance inflation factor (VIFs).

```{r}
corvif(subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber)))
```
Shows big GVIF (>4) for Pclass, Sex, CabinLetter, Title. Alain F. Zuur suggested to just drop predictors with high VIFs from the highest one by one.
```{r}
corvif(subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber,Title)))
```
Now dropping CabinLetter.
```{r}
corvif(subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber,Title, CabinLetter)))
```
Now all looks well in terms of VIFs.
```{r}
glmfit = glm(Survived~.,data=subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber,Title,CabinLetter)),family=binomial);
summary(glmfit)
```
Much better.

Let's create glm ignoring these columns using whole training data and try to check what predictors are the most reliable.
```{r}
glmfit = glm(Survived~.,data=subset(cTrain,select=-c(Surname,Cabin,Name,CabinNumber,Title,CabinLetter)),family=binomial);
summary(glmfit)
```
We see that to the reliable predictors that we saw on partial data glm trained on the whole data added a few more - Sibsp and Pclass.
VIFs are also ok for the whole data.
```{r}
corvif(subset(cTrain,select=-c(Surname,Cabin,Name,CabinNumber,Title,CabinLetter)))
```

It is also useful to make predictors vs response plot and look for visual trends and non-linearities. Again, quoting [A protocol for data exploration to avoid common statistical problems](http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2009.00001.x/full) "Note that the absence of clear patterns does not mean that there are no relationships; it just means that there are no clear two-way relationships. A model with multiple explanatory variables may still provide a good fit."
```{r}
plotPredictors('Survived',subset(cTrain,select=-c(Surname,Cabin,Name,CabinNumber,Title,CabinLetter)))
``` 

Plot glm predictions to interpret every predictor.

```{r}
library(effects)
plot(allEffects(glmfit))
```

We can also consider interaction of predictors.
```{r}
# Stepwise Regression
library(MASS)
glmfit = glm(Survived~.^2, data=subset(cTrain,select=-c(Surname,Cabin,Name,CabinNumber,Title,CabinLetter)), family=binomial);
step <- stepAIC(glmfit, direction="both")
step$anova # display results
plot(allEffects(glmfit))
```
However I cannot interpret interaction in this case, so we will end up with conclusion that the significant predictors are Sex, Age, Pclass. With plots showing slope and confidence intervals showed above.

## Using GLM for prediction
In this section we will use glm to make predictions and will not think about interpritability of the model. 

Though we are not going to use CabinLetter in determing reliabile predictors, we can still use it for overall predictions - [multicollinearity do not make predictions of lm worse](http://blog.minitab.com/blog/adventures-in-statistics/what-are-the-effects-of-multicollinearity-and-when-can-i-ignore-them), it just concerns interpretation of predictors coefficients. 

Lets look how odd its distribution
```{r}
table(data.frame(Pclass=cTrain$Pclass, CabinLetter=cTrain$CabinLetter))

```
It seems that Cabin data is not representative as we have it mostly for first class passengers. Thus we nevertheless are not going to impute and use it.

Let's try to make glm with all avaliable predictors.
```{r}
#glmfit = glm(Survived~.^2, data=cTrain, family=binomial);
```

```{r}
# Stepwise Regression
#library(MASS)
#glmfit = glm(Survived~.^2, data=cTrain, family=binomial);
#step <- stepAIC(glmfit, direction="both")
#step$anova # display results
```

Again plot glm predictions
```{r}
#library(effects)
#plot(allEffects(glmfit))
```
