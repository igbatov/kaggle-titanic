---
title: "Titanic data exploration using GLM, LDA, SVN, random forests"
author: "Igor Batov"
date: "August 28, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
First of all we want to extract the most influental predictors from GLM point of view.
Craft some new predictors using our common sense and create subset that has all predictors filled with data. 

```{r cars}
# for corvif() function
source("http://www.highstat.com/Book2/HighstatLibV6.R")

# for ggpairs() function
library(GGally)

# Load data
train <- read.csv('train.csv')
# 891 obs.
dim(train)

# Clean #1 
# -  remove NA cells
cTrain = na.omit(train)
# 891 obs.
dim(cTrain)
# - remove unnecesarry predictors
cTrain = subset(cTrain, select = -c(PassengerId,Ticket) )
# - set right predictor types
str(cTrain)
cTrain$Survived = factor(cTrain$Survived)
cTrain$Pclass = factor(cTrain$Pclass)

## Create new predictors
#  - extract Deck from Cabin
cTrain$CabinLetter = factor(substr(cTrain$Cabin, 1, 1))
#  - extract cabin number from Cabin
cTrain$CabinNumber = as.numeric(substr(cTrain$Cabin, 2, 4))
#  - extract title
cTrain$Title = factor(gsub('(.*, )|(\\..*)', '', cTrain$Name))
# - extract Surnames
cTrain$Surname <- factor(sapply(cTrain$Name, function(x){x=as.character(x); strsplit(x, split = '[,.]')[[1]][1];}))


# Clean #2 
# - remove all rows with empty Deck
ccTrain = subset(cTrain, CabinLetter != '')
dim(ccTrain)
# - remove all not-numeric CabinNumber
ccTrain = subset(ccTrain, !is.na(CabinNumber))
dim(ccTrain)
# - remove all not-numeric CabinNumber
ccTrain = subset(ccTrain, Embarked != '')
dim(ccTrain)
# - make CabinNumber to be a factor
ccTrain$CabinNumber = factor(ccTrain$CabinNumber);
```

Ok, we have trimmed out dataset from 891 observations to 176. The most truncating predictor was the CabinLetter. Using GLM we will try to determine if CabinLetter is important and there is need to impute it or we can safely ignore it completely.

```{r}
glmfit = glm(Survived~.,data=subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber)),family=binomial);
```
This gives use Error. Lets look what variables can cause this. The suspicious looks CabinNumber with 86 levels from 176 observations.
```{r}
levels(ccTrain$CabinNumber)
```
Let's exclude this from predictors.

```{r}
glmfit = glm(Survived~.,data=subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber)),family=binomial);

summary(glmfit)
```
This gives no confident coefficients for any variable. This can be because of predictors multicollinearity. To quote [A protocol for data exploration to avoid common statistical problems](http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2009.00001.x/full) "If collinearity is ignored, one is likely to end up with a confusing statistical analysis in which nothing is significant, but where dropping one covariate can make the others significant, or even change the sign of estimated parameters." Lets check variance inflation factor (VIFs).

```{r}
corvif(subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber)))
```
Shows big GVIF (>4) for Pclass, Sex, CabinLetter, Title. Alain F. Zuur suggested to just drop predictors with high VIFs from the highest one by one.
```{r}
corvif(subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber,Title)))
```
Now dropping CabinLetter.
```{r}
corvif(subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber,Title, CabinLetter)))
```
Now all looks well in terms of VIFs.
```{r}
glmfit = glm(Survived~.,data=subset(ccTrain,select=-c(Surname,Cabin,Name,CabinNumber,Title,CabinLetter)),family=binomial);
summary(glmfit)
```
Much better.

Let's create glm ignoring these columns using whole training data and try to check what predictors are the most reliable.
```{r}
glmfit = glm(Survived~.,data=subset(cTrain,select=-c(Surname,Cabin,Name,CabinNumber,Title,CabinLetter)),family=binomial);
summary(glmfit)
```
We see that to the reliable predictors that we saw on partial data glm trained on the whole data added a few more - Sibsp and Pclass.
VIFs are also ok for the whole data.
```{r}
corvif(subset(cTrain,select=-c(Surname,Cabin,Name,CabinNumber,Title,CabinLetter)))
```

It is also usually interesting to look at predictors vs respose plot for every predictor and look maybe for strange things on it. Again, quoting [A protocol for data exploration to avoid common statistical problems](http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2009.00001.x/full) "Note that the absence of clear patterns does not mean that there are no relationships; it just means that there are no clear two-way relationships. A model with multiple explanatory variables may still provide a good fit."
```{r}
plotPredictors('Survived',subset(cTrain,select=-c(Surname,Cabin,Name,CabinNumber,Title,CabinLetter)))
``` 


Though we are not going to use CabinLetter in determing reliabile predictors, [multicollinearity do not make predictions of lm worse](http://blog.minitab.com/blog/adventures-in-statistics/what-are-the-effects-of-multicollinearity-and-when-can-i-ignore-them), just interpretation of coefficients. So we will impute CabinsLetters and try to predict with lm.

